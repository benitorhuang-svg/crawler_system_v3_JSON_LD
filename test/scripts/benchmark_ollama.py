"""
å°ˆæ¡ˆåç¨±ï¼šcrawler_system_v3_JSON_LD
æ¨¡çµ„åç¨±ï¼šbenchmark_ollama.py
åŠŸèƒ½æè¿°ï¼šAI æå–æ•ˆèƒ½åŸºæº–æ¸¬è©¦å·¥å…·ï¼Œç”¨æ–¼å°ç…§ Ground Truth (L1) è©•ä¼° Ollama æ¨¡å‹çš„æå–æº–ç¢ºæ€§èˆ‡å»¶é²ã€‚
ä¸»è¦å…¥å£ï¼šuv run python test/scripts/benchmark_ollama.py
"""
import asyncio
import json
import os
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
import structlog
from core.enrichment.ollama_client import OllamaClient

logger = structlog.get_logger(__name__)

class OllamaBenchmarker:
    """
    SDD éšæ®µ 4ï¼šAI èƒ½åŠ›é©—è­‰èˆ‡å“è³ªç›£æ§å·¥å…·ã€‚
    
    é€éæ‰¹æ¬¡è™•ç†æ¨™ç«¿æ¨£æœ¬ï¼Œè¨ˆç®— Ollama åœ¨å¯¦é«”æå–ä»»å‹™ä¸­çš„å„é …æŒ‡æ¨™ï¼ˆæº–ç¢ºç‡ã€å»¶é²ï¼‰ã€‚
    æ­¤å·¥å…·ç¢ºä¿ AI çµ„ä»¶ç¬¦åˆç³»çµ±è¦æ ¼ã€‚
    """
    def __init__(self) -> None:
        """åˆå§‹åŒ–æ¸¬è©¦ç’°å¢ƒï¼Œé…ç½®è·¯å¾‘èˆ‡ Ollama å®¢æˆ¶ç«¯ã€‚"""
        self.client: OllamaClient = OllamaClient()
        self.base_dir: Path = Path(__file__).parent.parent.parent
        self.data_dir: Path = self.base_dir / "test" / "fixtures" / "data"
        self.results_dir: Path = self.base_dir / "test" / "unit" / "debug" / "benchmarks"
        self.results_dir.mkdir(parents=True, exist_ok=True)

    def _calculate_score(self, truth: Dict[str, Any], extracted: Dict[str, Any]) -> Dict[str, float]:
        """
        è¨ˆç®—å–®ä¸€æå–çµæœèˆ‡æ¨™æº–ç­”æ¡ˆçš„åŒ¹é…å¾—åˆ†ã€‚
        """
        metrics: Dict[str, float] = {}
        fields: List[str] = ["title", "company_name", "salary_text", "salary_type"]
        
        for field in fields:
            t_val: str = str(truth.get(field, "")).strip().lower()
            e_val: str = str(extracted.get(field, "")).strip().lower()
            
            # å¯¬é¬†åŒ¹é…é‚è¼¯ (Substring åŒ¹é…)
            if t_val == e_val and t_val != "":
                metrics[field] = 1.0
            elif t_val in e_val and t_val != "":
                metrics[field] = 0.8
            elif t_val == "" and e_val == "":
                metrics[field] = 1.0
            else:
                metrics[field] = 0.0
                
        metrics["total"] = sum(metrics.values()) / len(fields)
        return metrics

    async def run_benchmark(self, limit: int = 20) -> None:
        """
        å°æŒ‡å®šæ•¸é‡çš„æ¨£æœ¬åŸ·è¡Œå¾ªç’°æå–æ¸¬è©¦ã€‚
        
        Args:
            limit (int): æœ€å¤§æ¸¬è©¦æ¨£æœ¬æ•¸ã€‚
        """
        print(f"\nğŸš€ Ollama åŸºæº–æ¸¬è©¦å•Ÿå‹• (æ¨¡å‹: {self.client.model})")
        print(f"ğŸ“ æ•¸æ“šä¾†æº: {self.data_dir}")
        
        # 1. è­˜åˆ¥ JSON æ ¼å¼çš„ Ground Truth æª”æ¡ˆ
        samples: List[Path] = [p for p in self.data_dir.glob("*.json") if "metadata" not in p.name]
        if not samples:
            print("âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æ¸¬è©¦æ¨£æœ¬ (JSON)ã€‚")
            return

        total_metrics: Dict[str, float] = {"title": 0.0, "company_name": 0.0, "salary_text": 0.0, "salary_type": 0.0, "total": 0.0}
        count: int = 0

        for sample_path in samples[:limit]:
            with open(sample_path, "r", encoding="utf-8") as f:
                truth_data: Dict[str, Any] = json.load(f)
            
            # å°‹æ‰¾å°æ‡‰çš„ HTML æª”æ¡ˆ
            html_path: Path = sample_path.with_suffix(".html")
            if not html_path.exists():
                continue

            with open(html_path, "r", encoding="utf-8") as f:
                html_content: str = f.read()

            print(f" [+] è©•æ ¸ä¸­: {sample_path.name}")
            
            start_t: float = time.perf_counter()
            # å‘¼å« AI é€²è¡Œèªç¾©æå–
            extracted: Dict[str, Any] = await self.client.extract_job_from_html(html_content)
            latency: float = time.perf_counter() - start_t

            scores: Dict[str, float] = self._calculate_score(truth_data, extracted)
            for k, v in scores.items():
                if k in total_metrics:
                    total_metrics[k] += v
            
            count += 1
            print(f"     â” æº–ç¢ºåº¦: {scores['total']:.1%}, è€—æ™‚: {latency:.2f}s")

        if count > 0:
            avg_metrics: Dict[str, float] = {k: v / count for k, v in total_metrics.items()}
            print("\nğŸ“Š æ¸¬è©¦å½™æ•´å ±å‘Š")
            for k, v in avg_metrics.items():
                print(f" {k:15}: {v:.2%}")
            
            # æŒä¹…åŒ–å ±å‘Š
            ts: int = int(time.time())
            report: Dict[str, Any] = {
                "timestamp": ts,
                "model": self.client.model,
                "count": count,
                "metrics": avg_metrics
            }
            report_path: Path = self.results_dir / f"report_{ts}.json"
            with open(report_path, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            print(f"\nâœ… åŸºæº–æ¸¬è©¦å®Œæˆï¼Œå ±å‘Šå·²å­˜è‡³ {report_path}")
        else:
            print("âš ï¸ æ‰¾ä¸åˆ°æœ‰æ•ˆçš„ (JSON+HTML) æ¨£æœ¬å°ã€‚")

if __name__ == "__main__":
    bench = OllamaBenchmarker()
    asyncio.run(bench.run_benchmark())
