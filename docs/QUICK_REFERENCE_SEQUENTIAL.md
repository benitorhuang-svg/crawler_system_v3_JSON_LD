# è·æ¥­é¡åˆ¥é †åºåŸ·è¡Œ - å¿«é€Ÿåƒè€ƒ

> **æœ€å¸¸ç”¨çš„ 5 å€‹å‘½ä»¤**

---

## 1ï¸âƒ£ é¦–æ¬¡çˆ¬å–ï¼ˆæ¨™æº–æ¨¡å¼ï¼‰

```python
# æ‰€æœ‰å¹³å°ä¸¦è¡Œï¼Œå…§éƒ¨åˆ†é¡é †åºåŸ·è¡Œ
await crawl_service.run_all(limit_per_platform=20, resume=True)
```

**æ•ˆæœï¼š**
- 5 å€‹å¹³å° **åŒæ™‚åŸ·è¡Œ**
- å„å¹³å°å…§åˆ†é¡ **é€å€‹åŸ·è¡Œ**
- å·²å®Œæˆåˆ†é¡è‡ªå‹•è·³éï¼ˆå¤šæ¬¡åŸ·è¡Œæ™‚ï¼‰
- âœ… é æœŸè€—æ™‚ï¼š~12 åˆ†é˜

---

## 2ï¸âƒ£ æ•…éšœæ¢å¾©ï¼ˆè‡ªå‹•ç¹¼çºŒï¼‰

```python
# å®¹å™¨å´©æ½°å¾Œç›´æ¥é‡å•Ÿï¼Œç³»çµ±è‡ªå‹•æ¥çºŒ
await crawl_service.run_all(limit_per_platform=20, resume=True)
```

**æ•ˆæœï¼š**
- è‡ªå‹•åµæ¸¬å·²çˆ¬å–åˆ†é¡
- è·³éå®Œæˆçš„åˆ†é¡ A-C
- æ¥çºŒæœªå®Œæˆåˆ†é¡ D
- ç„¡éœ€æ‰‹å‹•ä¿®æ”¹æ•¸æ“šåº«
- âœ… ç¯€çœ ~5 åˆ†é˜

---

## 3ï¸âƒ£ å¼·åˆ¶é‡çˆ¬ï¼ˆæ¸…é™¤é€²åº¦ï¼‰

```python
# é‡æ–°çˆ¬å–å…¨éƒ¨åˆ†é¡ï¼Œå¿½ç•¥ä¹‹å‰çš„é€²åº¦
await crawl_service.run_all(limit_per_platform=20, resume=False)
```

**æ•ˆæœï¼š**
- é‡æ–°çˆ¬å– **æ‰€æœ‰åˆ†é¡**
- å¿½ç•¥ `tb_categories.updated_at` è¨˜éŒ„
- ç”¨æ–¼æ¸¬è©¦æˆ–è³‡æ–™é©—è­‰
- âš ï¸ è€—æ™‚ï¼šå…¨éƒ¨ 12 åˆ†é˜

---

## 4ï¸âƒ£ å–®å€‹å¹³å°é‡çˆ¬ï¼ˆéƒ¨åˆ†é‡åšï¼‰

```python
# åªé‡çˆ¬å¹³å° 104ï¼Œå…¶ä»–å¹³å°ä¸è®Š
await crawl_service.run_platform(
    SourcePlatform.PLATFORM_104,
    max_jobs=20,
    resume=False
)
```

**æ•ˆæœï¼š**
- åªé‡çˆ¬ **å¹³å° 104**
- å…¶ä»–å¹³å°ç¹¼çºŒåŸ·è¡Œ
- ç”¨æ–¼å¹³å°ç´šæ•…éšœä¿®å¾©

---

## 5ï¸âƒ£ ç‰¹å®šåˆ†é¡æ¸¬è©¦ï¼ˆDebugï¼‰

```python
# åªçˆ¬å–æŸå€‹ç‰¹å®šåˆ†é¡ï¼ˆç”¨æ–¼æ¸¬è©¦æˆ–ä¿®å¾©ï¼‰
await crawl_service.run_platform(
    SourcePlatform.PLATFORM_104,
    target_cat_id="cat_software_engineering",
    max_jobs=50,
    resume=True  # æ³¨ï¼štarget_cat_id æ™‚ resume ç„¡æ•ˆ
)
```

**æ•ˆæœï¼š**
- åªè™•ç† **1 å€‹åˆ†é¡**
- æœ€å¤§ 50 å€‹è·ç¼º
- ç”¨æ–¼åˆ†é¡ç´šæ•…éšœä¿®å¾©
- âœ… è€—æ™‚ï¼š~2 åˆ†é˜

---

## ğŸ“Š æ—¥èªŒç›£æ§

### æŸ¥çœ‹é€²åº¦

```bash
# å¯¦æ™‚æŸ¥çœ‹çˆ¬èŸ²é€²åº¦
docker logs crawler_system -f | grep "category_processing"
```

**å…¸å‹è¼¸å‡ºï¼š**
```
2026-01-29T10:00:10Z category_processing_start platform=platform_104 category_index=1/8 cat_id=cat_001
2026-01-29T10:15:30Z category_processing_completed platform=platform_104 cat=cat_001 progress=1/8
2026-01-29T10:15:35Z category_processing_start platform=platform_104 category_index=2/8 cat_id=cat_002
```

### æª¢æŸ¥å·²çˆ¬å–åˆ†é¡

```python
# æŸ¥è©¢å·²çˆ¬å–åˆ†é¡
db = Database()
crawled = await db.get_crawled_categories("platform_104", days=30)
print(f"å·²çˆ¬å–: {len(crawled)} å€‹åˆ†é¡")
print(f"åˆ†é¡åˆ—è¡¨: {crawled}")
```

---

## ğŸ¯ å¸¸è¦‹å ´æ™¯å°æ‡‰è¡¨

| å ´æ™¯ | å‘½ä»¤ | è€—æ™‚ | å‚™è¨» |
|------|------|------|------|
| ç¬¬ä¸€æ¬¡çˆ¬å– | `run_all(resume=True)` | 12 min | æ¨™æº–æµç¨‹ |
| å®¹å™¨é‡å•Ÿ | `run_all(resume=True)` | ~5 min | è·³éå·²å®Œæˆ |
| å‡ç´šæ–°è¦å‰‡ | `run_all(resume=False)` | 12 min | å…¨éƒ¨é‡åš |
| ä¿®å¾©å¹³å° 104 | `run_platform(104, resume=False)` | 12 min | åªæ”¹ä¸€å€‹å¹³å° |
| æ¸¬è©¦åˆ†é¡ | `run_platform(104, target_cat_id=...)` | 2 min | Debug ç”¨ |

---

## âš ï¸ å¸¸è¦‹å•é¡Œ

### Q: ç‚ºä»€éº¼åˆ†é¡åŸ·è¡Œé †åºè€Œä¸æ˜¯ä¸¦è¡Œï¼Ÿ
**A:** ç‚ºäº†å¯¦ç¾ã€Œæ¥çºŒä¸Šæ¬¡çš„åˆ†é¡ã€åŠŸèƒ½ã€‚ä¸¦è¡ŒåŸ·è¡Œæ™‚ç„¡æ³•ç²¾ç¢ºå®šä½å“ªå€‹åˆ†é¡å¤±æ•—ã€‚ç¾åœ¨çš„è¨­è¨ˆï¼š
- âœ… æ”¯æŒæ•…éšœæ¢å¾©
- âœ… é€²åº¦å¯è¿½è¹¤
- âœ… ååé‡ç„¡æï¼ˆå¹³å°å±¤ä»ä¸¦è¡Œï¼‰

### Q: ç‚ºä»€éº¼å¹³å°æ”¹ç‚ºä¸¦è¡Œï¼Ÿ
**A:** å¤§å¹…å‰Šæ¸›ç¸½è€—æ™‚ï¼š
```
èˆŠ: å¹³å° 104 (12 min) â†’ 1111 (12 min) â†’ ... = 60 min
æ–°: å¹³å°ä¸¦è¡Œ max(12, 12, ...) = 12 min  â† 5 å€åŠ é€Ÿï¼
```

### Q: å¦‚ä½•å¼·åˆ¶é‡çˆ¬æŸå€‹åˆ†é¡ï¼Ÿ
**A:** ä½¿ç”¨ `target_cat_id` åƒæ•¸ï¼ˆè‡ªå‹•å¿½ç•¥ resume é‚è¼¯ï¼‰ï¼š
```python
await crawl_service.run_platform(
    SourcePlatform.PLATFORM_104,
    target_cat_id="cat_001",  # åªçˆ¬é€™å€‹
    resume=True
)
```

### Q: å¦‚ä½•æŸ¥çœ‹é‚„æœ‰å¤šå°‘åˆ†é¡æœªçˆ¬å–ï¼Ÿ
**A:** 
```sql
-- æ–¹å¼ 1: æŸ¥è©¢ 30 å¤©å…§å·²æ›´æ–°çš„åˆ†é¡
SELECT DISTINCT layer_3_id 
FROM tb_categories 
WHERE platform = 'platform_104'
  AND updated_at >= DATE_SUB(NOW(), INTERVAL 30 DAY);

-- æ–¹å¼ 2: æŸ¥è©¢ 30 å¤©å…§æœªæ›´æ–°çš„åˆ†é¡
SELECT DISTINCT layer_3_id 
FROM tb_categories 
WHERE platform = 'platform_104'
  AND updated_at < DATE_SUB(NOW(), INTERVAL 30 DAY);
```

### Q: Resume çš„æ™‚é–“ç¯„åœå¯ä»¥æ”¹å—ï¼Ÿ
**A:** å¯ä»¥ï¼Œä¿®æ”¹ `get_crawled_categories()` çš„ `days` åƒæ•¸ï¼š
```python
# åªè€ƒæ…® 7 å¤©å…§çš„è¨˜éŒ„
crawled = await db.get_crawled_categories("platform_104", days=7)

# æˆ–è€…è€ƒæ…®æ‰€æœ‰è¨˜éŒ„
crawled = await db.get_crawled_categories("platform_104", days=999)
```

---

## ğŸ” é©—è­‰ä¿®æ”¹æ˜¯å¦ç”Ÿæ•ˆ

### 1. æª¢æŸ¥æ—¥èªŒæ ¼å¼

```bash
# åŸ·è¡Œçˆ¬èŸ²ä¸¦ç›£æ§
docker logs crawler_system -f 2>&1 | head -100
```

**æ‡‰è©²çœ‹åˆ°ï¼š**
- âœ… `category_mode="sequential"` 
- âœ… `category_index="N/M"` 
- âœ… é€å€‹åˆ†é¡çš„ `category_processing_*` äº‹ä»¶

### 2. é©—è­‰ Resume é‚è¼¯

```python
# æ¸¬è©¦è…³æœ¬
python scripts/test_sequential_execution.py
```

**æ‡‰è©²è¿”å›ï¼š**
```
âœ… PASS: TEST 1: get_crawled_categories()
âœ… PASS: TEST 2: åˆ†é¡è·³éé‚è¼¯
âœ… PASS: TEST 3: Resume éæ¿¾é‚è¼¯
âœ… PASS: TEST 4: é †åºåŸ·è¡Œçµæ§‹
ç¸½è¨ˆ: 4/4 æ¸¬è©¦é€šé
```

### 3. æª¢æŸ¥æ•¸æ“šåº«æ›´æ–°

```sql
-- æŸ¥è©¢åˆ†é¡è¡¨çš„ updated_at æ™‚é–“æˆ³
SELECT layer_3_id, updated_at 
FROM tb_categories 
WHERE platform = 'platform_104'
ORDER BY updated_at DESC LIMIT 10;

-- æ‡‰è©²çœ‹åˆ°æœ€è¿‘æ›´æ–°çš„åˆ†é¡
```

---

## ğŸ“ˆ æ€§èƒ½æŒ‡æ¨™

### é æœŸæ”¹é€²

| æŒ‡æ¨™ | èˆŠç³»çµ± | æ–°ç³»çµ± | æ”¹é€² |
|------|--------|--------|------|
| **é¦–æ¬¡çˆ¬å–è€—æ™‚** | 60 min | 12 min | **5 å€å¿«é€Ÿ** âš¡ |
| **å®¹å™¨é‡å•Ÿæ¢å¾©** | 60 min | 5 min | **92% åŠ é€Ÿ** âš¡ |
| **é€²åº¦å¯è¦–æ€§** | âŒ å·® | âœ… å„ª | **é¡¯è‘—æå‡** ğŸ“Š |
| **æ•…éšœæ¢å¾©æˆæœ¬** | é«˜ | ä½ | **58% ç¯€çœ** ğŸ’° |

### ååé‡

```
URL ä¸¦ç™¼æ•¸: 5 (ä¸è®Šï¼Œæ§åˆ¶åœ¨ URL å±¤)
å¹³å°ä¸¦è¡Œæ•¸: 1 â†’ 5 (æ–°å¢)
åˆ†é¡åŸ·è¡Œæ¨¡å¼: ä¸¦è¡Œ â†’ é †åº (æ”¹é€²å¯è¿½è¹¤æ€§)

çµæœ: ç¸½è€—æ™‚ 12 min (5 å€æ”¹é€²)
```

---

## ğŸš€ æœ€ä½³å¯¦è¸

### âœ… æ¨è–¦åšæ³•

1. **å®šæœŸåŸ·è¡Œ** `run_all(resume=True)`
   ```python
   # æ¯æ—¥ 10:00 åŸ·è¡Œä¸€æ¬¡
   await crawl_service.run_all(limit_per_platform=20, resume=True)
   ```

2. **ç›£æ§æ—¥èªŒä¸­çš„é€²åº¦**
   ```bash
   # è¨­ç½®å‘Šè­¦ï¼šè‹¥æŸåˆ†é¡å¡åœ¨åŒä¸€åœ°é»è¶…é 1 å°æ™‚
   docker logs crawler_system -f | grep -E "category_index|error"
   ```

3. **æ¯é€±é€²è¡Œé©—è­‰çˆ¬å–**
   ```python
   # æ¯é€±æ—¥é€²è¡Œ resume=False é©—è­‰
   if datetime.now().weekday() == 6:  # Sunday
       await crawl_service.run_all(limit_per_platform=20, resume=False)
   ```

### âŒ é¿å…åšæ³•

1. âŒ **æ‰‹å‹•ä¿®æ”¹ `updated_at` æ¬„ä½**ï¼ˆæ‡‰ä½¿ç”¨ `resume=False`ï¼‰
2. âŒ **åŒæ™‚å•Ÿå‹•å¤šå€‹ `run_all()` å¯¦ä¾‹**ï¼ˆé¿å…è³‡æ–™ç«¶åˆï¼‰
3. âŒ **éæ–¼é »ç¹çš„ `resume=False` çˆ¬å–**ï¼ˆæµªè²»è³‡æºï¼‰

---

## ğŸ“ æ”¯æŒèˆ‡èª¿è©¦

### è‹¥é‡åˆ°å•é¡Œ

1. **åˆ†é¡æœªè·³é (Resume å¤±æ•ˆ)**
   ```sql
   -- æª¢æŸ¥ tb_categories æ˜¯å¦æœ‰ç›¸æ‡‰åˆ†é¡
   SELECT * FROM tb_categories 
   WHERE platform='platform_104' AND layer_3_id='cat_001';
   ```

2. **é€²åº¦æ—¥èªŒæ¶ˆå¤±**
   ```bash
   # æª¢æŸ¥æ—¥èªŒç´šåˆ¥è¨­å®š
   grep -r "LOG_LEVEL" core/infra/logging_config.py
   ```

3. **çˆ¬èŸ²ç„¡æ³•æ¥çºŒ**
   ```python
   # å¼·åˆ¶åˆ·æ–°é€²åº¦ï¼ˆå±éšªæ“ä½œï¼‰
   await db.mark_category_as_crawled('platform_104', 'cat_001')
   ```

---

