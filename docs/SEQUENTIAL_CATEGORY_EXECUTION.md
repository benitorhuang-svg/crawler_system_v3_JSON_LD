# è·æ¥­é¡åˆ¥é †åºåŸ·è¡Œ - ä¿®æ­£æ–¹æ¡ˆ

> **æ—¥æœŸ**: 2026-01-29  
> **ç›®æ¨™**: æ”¹ç‚ºé€å€‹è·æ¥­é¡åˆ¥åŸ·è¡Œï¼Œæ”¯æŒæš«åœ/æ¢å¾©

---

## ä¿®æ­£æ¦‚è¿°

### ç•¶å‰å•é¡Œ

```python
# ç¾æœ‰å¯¦ç¾ï¼šæ‰€æœ‰åˆ†é¡ä¸¦è¡ŒåŸ·è¡Œ
cat_tasks = [process_category(cat) for cat in categories]
await asyncio.gather(*cat_tasks)  # âš ï¸ æ‰€æœ‰åˆ†é¡åŒæ™‚åŸ·è¡Œ
```

å•é¡Œï¼š
- âŒ ç„¡æ³•ç´°ç²’åº¦è¿½è¹¤é€²åº¦ï¼ˆæŸåˆ†é¡å¡åˆ°æŸå€‹ URLï¼‰
- âŒ æ•…éšœæ¢å¾©æ™‚ç„¡æ³•æ¥çºŒä¸Šæ¬¡çš„åˆ†é¡
- âŒ ä¸¦è¡ŒåŸ·è¡Œæœƒå°è‡´åŒä¸€å¹³å°ä¸Šæœ‰å¤šå€‹åˆ†é¡æ¶äº’è¯ç¶²é »å¯¬

### å„ªåŒ–æ–¹æ¡ˆ

```python
# ä¿®æ­£å¯¦ç¾ï¼šé€å€‹åˆ†é¡åŸ·è¡Œ
for cat in categories:
    await process_category(cat)  # âœ… ä¸€å€‹æ¥ä¸€å€‹
    # æ¯å€‹åˆ†é¡å®Œæˆå¾Œè‡ªå‹•è¨˜éŒ„é€²åº¦
```

å„ªé»ï¼š
- âœ… æ¸…æ™°çš„é€²åº¦è¿½è¹¤ï¼ˆå®Œæˆåˆ°ç¬¬ N å€‹åˆ†é¡ï¼‰
- âœ… æ”¯æŒæš«åœ/æ¢å¾©ï¼ˆæ•…éšœå¾Œå¯æ¥çºŒä¸Šæ¬¡ï¼‰
- âœ… é †åºåŸ·è¡Œï¼Œå…·æœ‰å¯é æ¸¬æ€§

---

## ä»£ç¢¼ä¿®æ­£

### ä¿®æ”¹ 1: run_platform() - æ”¹ç‚ºé€å€‹åˆ†é¡åŸ·è¡Œ

**ä½ç½®**: `core/services/crawl_service.py` (L420-454)

**ç•¶å‰ä»£ç¢¼:**
```python
async def run_platform(self, platform: SourcePlatform, max_jobs: int = 20, target_cat_id: Optional[str] = None) -> None:
    """åŸ·è¡Œç‰¹å®šå¹³å°çš„çˆ¬å–æµæ°´ç·šã€‚"""
    logger.info("pipeline_started", platform=platform.value, cat_limit=target_cat_id)
    
    categories: List[Dict[str, Any]] = await self.discovery.get_category_codes(platform, target_id=target_cat_id)
    
    async with httpx.AsyncClient(verify=False, follow_redirects=True, timeout=20.0) as client:
        # è¨­ç½®ä½µç™¼é™åˆ¶
        sem = asyncio.Semaphore(5)
        
        async def process_category(cat: Dict[str, Any]):
            cat_id: str = cat["layer_3_id"]
            cat_name: Optional[str] = cat.get("layer_3_name")
            
            # æ¢ç´¢ URL åˆ—è¡¨
            urls: List[str] = await self.discovery.discover_category(platform, cat_id, client, limit=max_jobs)
            if not urls: return
            
            logger.info("category_discovery_stats", platform=platform.value, cat=cat_id, count=len(urls))
            
            # ä½µç™¼è™•ç†è©²é¡åˆ¥ä¸‹çš„ç¶²å€
            async def process_with_sem(url: str):
                async with sem:
                    await self._process_url_and_save(platform, url, client, cat_id, cat_name)

            job_tasks = [process_with_sem(url) for url in list(set(urls))[:max_jobs]]
            await asyncio.gather(*job_tasks)
            await self.db.mark_category_as_crawled(platform.value, cat_id)

        # åŸ·è¡Œæ‰€æœ‰é¡åˆ¥çš„è™•ç†
        cat_tasks = [process_category(cat) for cat in categories]
        await asyncio.gather(*cat_tasks)
```

**ä¿®æ­£å¾Œä»£ç¢¼:**
```python
async def run_platform(
    self, 
    platform: SourcePlatform, 
    max_jobs: int = 20, 
    target_cat_id: Optional[str] = None,
    resume: bool = True  # âœ… æ–°å¢ï¼šæ˜¯å¦æ¢å¾©ä¸Šæ¬¡é€²åº¦
) -> None:
    """
    åŸ·è¡Œç‰¹å®šå¹³å°çš„çˆ¬å–æµæ°´ç·šï¼ˆé€å€‹è·æ¥­é¡åˆ¥é †åºåŸ·è¡Œï¼‰ã€‚
    
    Args:
        platform (SourcePlatform): ç›®æ¨™å¹³å°ã€‚
        max_jobs (int): æ¯å€‹åˆ†é¡çš„è·ç¼ºä¸Šé™ã€‚
        target_cat_id (Optional[str]): è‹¥æŒ‡å®šï¼Œåªçˆ¬å–è©²åˆ†é¡ã€‚
        resume (bool): è‹¥ Trueï¼Œè·³éå·²å®Œæˆçš„åˆ†é¡ï¼›è‹¥ Falseï¼Œé‡æ–°è™•ç†å…¨éƒ¨ã€‚
    """
    logger.info("pipeline_started", platform=platform.value, category_mode="sequential", resume=resume)
    
    categories: List[Dict[str, Any]] = await self.discovery.get_category_codes(platform, target_id=target_cat_id)
    if not categories:
        logger.warning("no_categories_found", platform=platform.value)
        return
    
    # âœ… è‹¥ resume=Trueï¼Œéæ¿¾æ‰å·²å®Œæˆçš„åˆ†é¡
    if resume and not target_cat_id:
        crawled_cats = await self.db.get_crawled_categories(platform.value)
        categories = [cat for cat in categories if cat["layer_3_id"] not in crawled_cats]
        logger.info("resume_mode_filtered", platform=platform.value, remaining=len(categories))
    
    async with httpx.AsyncClient(
        verify=False, 
        follow_redirects=True, 
        timeout=20.0,
        limits=httpx.Limits(max_connections=20, max_keepalive_connections=10)
    ) as client:
        # è¨­ç½®ä½µç™¼é™åˆ¶ï¼ˆURL ç´šåˆ¥ï¼Œä¸æ˜¯åˆ†é¡ç´šåˆ¥ï¼‰
        sem = asyncio.Semaphore(self._get_concurrency_for_platform(platform))
        
        # âœ… é€å€‹åˆ†é¡åŸ·è¡Œï¼ˆè€Œä¸æ˜¯ä¸¦è¡Œ gatherï¼‰
        total_cats = len(categories)
        for cat_idx, cat in enumerate(categories, 1):
            cat_id: str = cat["layer_3_id"]
            cat_name: Optional[str] = cat.get("layer_3_name")
            
            logger.info(
                "category_processing_start",
                platform=platform.value,
                category_index=f"{cat_idx}/{total_cats}",
                cat_id=cat_id,
                cat_name=cat_name
            )
            
            try:
                # æ¢ç´¢ URL åˆ—è¡¨
                urls: List[str] = await self.discovery.discover_category(
                    platform, 
                    cat_id, 
                    client, 
                    limit=max_jobs
                )
                
                if not urls:
                    logger.debug("category_no_urls", platform=platform.value, cat=cat_id)
                    await self.db.mark_category_as_crawled(platform.value, cat_id)
                    continue
                
                logger.info(
                    "category_discovery_stats",
                    platform=platform.value,
                    cat=cat_id,
                    count=len(urls)
                )
                
                # ä½µç™¼è™•ç†è©²é¡åˆ¥ä¸‹çš„ç¶²å€ï¼ˆä¿¡è™Ÿé‡æ‡‰ç”¨æ–¼ URL ç´šåˆ¥ï¼‰
                async def process_with_sem(url: str):
                    async with sem:
                        await self._process_url_and_save(
                            platform, 
                            url, 
                            client, 
                            cat_id, 
                            cat_name
                        )
                
                # âœ… æ”¹ç‚ºï¼šå»ºç«‹ä»»å‹™ä½†é †åºåŸ·è¡Œ URL
                job_tasks = [
                    process_with_sem(url) 
                    for url in list(set(urls))[:max_jobs]
                ]
                
                # åŸ·è¡Œæ‰€æœ‰ URLï¼ˆä½†å—ä¿¡è™Ÿé‡é™åˆ¶ï¼Œç¢ºä¿ä¸¦ç™¼åº¦æ§åˆ¶ï¼‰
                await asyncio.gather(*job_tasks, return_exceptions=True)
                
                # âœ… åˆ†é¡è™•ç†å®Œæˆï¼Œæ¨™è¨˜é€²åº¦
                await self.db.mark_category_as_crawled(platform.value, cat_id)
                
                logger.info(
                    "category_processing_completed",
                    platform=platform.value,
                    cat=cat_id,
                    progress=f"{cat_idx}/{total_cats}"
                )
                
            except Exception as e:
                logger.error(
                    "category_processing_error",
                    platform=platform.value,
                    cat=cat_id,
                    error=str(e),
                    exc_info=True
                )
                # âš ï¸ åˆ†é¡å¤±æ•—æ™‚ä¸æ¨™è¨˜ç‚ºå®Œæˆï¼Œä¸‹æ¬¡ resume æ™‚æœƒé‡è©¦
                continue
        
        logger.info("pipeline_completed", platform=platform.value, total_categories=total_cats)
```

---

### ä¿®æ”¹ 2: æ–°å¢è¼”åŠ©æ–¹æ³• - get_crawled_categories()

**ä½ç½®**: `core/infra/database.py` (æ–°å¢æ–¹æ³•)

```python
async def get_crawled_categories(self, platform: str, days: int = 30) -> set:
    """
    å–å¾—æŒ‡å®šå¹³å°å·²çˆ¬å–çš„åˆ†é¡åˆ—è¡¨ã€‚
    
    Args:
        platform (str): å¹³å°åç¨± (e.g., 'platform_104')ã€‚
        days (int): æŸ¥è©¢æ™‚é–“ç¯„åœï¼ˆå¤©æ•¸ï¼‰ã€‚
    
    Returns:
        set: å·²çˆ¬å–çš„åˆ†é¡ ID é›†åˆã€‚
    """
    try:
        async with self.safe_cursor() as cursor:
            await cursor.execute(
                """
                SELECT DISTINCT layer_3_id 
                FROM tb_categories 
                WHERE platform = %s 
                  AND updated_at >= DATE_SUB(NOW(), INTERVAL %s DAY)
                """,
                (platform, days)
            )
            rows = await cursor.fetchall()
            return {row[0] for row in rows}
    except Exception as e:
        logger.error("get_crawled_categories_failed", platform=platform, error=str(e))
        return set()
```

---

### ä¿®æ”¹ 3: run_all() - å¹³å°ä»ç„¶ä¸¦è¡Œï¼Œåˆ†é¡é †åºåŸ·è¡Œ

**ä½ç½®**: `core/services/crawl_service.py` (L456-462)

**ç•¶å‰ä»£ç¢¼:**
```python
async def run_all(self, limit_per_platform: int = 10) -> None:
    """å•Ÿå‹•æ‰€æœ‰æ”¯æ´å¹³å°çš„è‡ªå‹•æŠ“å–ã€‚"""
    for p in SourcePlatform:
        if p == SourcePlatform.PLATFORM_UNKNOWN: continue
        try:
            await self.run_platform(p, max_jobs=limit_per_platform)
        except Exception as e:
            logger.error("platform_crash", platform=p.value, error=str(e))
```

**ä¿®æ­£å¾Œä»£ç¢¼:**
```python
async def run_all(
    self, 
    limit_per_platform: int = 10,
    resume: bool = True  # âœ… æ–°å¢ï¼šæ”¯æŒæ¢å¾©ä¸Šæ¬¡é€²åº¦
) -> None:
    """
    å•Ÿå‹•æ‰€æœ‰æ”¯æ´å¹³å°çš„è‡ªå‹•æŠ“å–ï¼ˆå¹³å°ä¸¦è¡Œï¼Œåˆ†é¡é †åºï¼‰ã€‚
    
    Args:
        limit_per_platform (int): æ¯å€‹åˆ†é¡çš„è·ç¼ºä¸Šé™ã€‚
        resume (bool): è‹¥ Trueï¼Œè·³éå·²å®Œæˆçš„åˆ†é¡ã€‚
    """
    logger.info("run_all_started", mode="parallel_platforms_sequential_categories", resume=resume)
    
    # âœ… 5 å€‹å¹³å°ä¸¦è¡ŒåŸ·è¡Œï¼ˆå„è‡ªå…§éƒ¨è·æ¥­é¡åˆ¥é †åºåŸ·è¡Œï¼‰
    tasks = [
        self.run_platform(
            p, 
            max_jobs=limit_per_platform,
            resume=resume  # å‚³é resume åƒæ•¸
        )
        for p in SourcePlatform
        if p != SourcePlatform.PLATFORM_UNKNOWN
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    # çµ±è¨ˆçµæœ
    failures = sum(1 for r in results if isinstance(r, Exception))
    logger.info(
        "run_all_completed",
        total_platforms=len(results),
        failures=failures,
        mode="parallel_platforms_sequential_categories"
    )
    
    if failures > 0:
        logger.warning("run_all_had_failures", failed_platforms=failures)
```

---

## åŸ·è¡Œæ¨¡å¼å°æ¯”

### å„ªåŒ–å‰ (ä¸¦è¡Œæ‰€æœ‰åˆ†é¡)

```
å¹³å° 104
â”œâ”€ åˆ†é¡ A (ä¸¦è¡Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”œâ”€ åˆ†é¡ B (ä¸¦è¡Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€ åŒæ™‚åŸ·è¡Œï¼Œ12 åˆ†é˜
â”œâ”€ åˆ†é¡ C (ä¸¦è¡Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â””â”€ åˆ†é¡ D (ä¸¦è¡Œ) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å•é¡Œï¼š
âŒ ç„¡æ³•ç²¾ç¢ºè¿½è¹¤ã€Œå¡åœ¨åˆ†é¡ B çš„ URL 5ã€
âŒ æ•…éšœå¾Œç„¡æ³•æ¥çºŒã€Œåˆ†é¡ C çš„ URL 8ã€
```

### å„ªåŒ–å¾Œ (é †åºåŸ·è¡Œåˆ†é¡)

```
å¹³å° 104
â”œâ”€ åˆ†é¡ A [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] å®Œæˆï¼Œå·²è¨˜éŒ„
â”œâ”€ åˆ†é¡ B [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘] é€²è¡Œä¸­...
â”‚  â”œâ”€ URL 1 [å®Œæˆ]
â”‚  â”œâ”€ URL 2 [å®Œæˆ]
â”‚  â”œâ”€ URL 3 [é€²è¡Œä¸­...]
â”‚  â””â”€ ...
â”œâ”€ åˆ†é¡ C [å¾…åŸ·è¡Œ]
â””â”€ åˆ†é¡ D [å¾…åŸ·è¡Œ]

å¥½è™•ï¼š
âœ… æ¸…æ™°é€²åº¦è¿½è¹¤
âœ… æ•…éšœå¾Œå¯æ¥çºŒåˆ†é¡ B çš„ URL 3
âœ… æ”¯æŒ resume æ©Ÿåˆ¶
```

---

## ä½¿ç”¨å ´æ™¯

### å ´æ™¯ 1: å…¨æ–°çˆ¬å–

```python
# ç¬¬ä¸€æ¬¡çˆ¬å–ï¼Œresume=Trueï¼ˆé è¨­ï¼‰
await crawl_service.run_all(limit_per_platform=10, resume=True)

# å…¨éƒ¨åˆ†é¡é€å€‹åŸ·è¡Œ
# æ¯å®Œæˆä¸€å€‹åˆ†é¡å°±æ¨™è¨˜é€²åº¦
```

### å ´æ™¯ 2: æ•…éšœæ¢å¾©

```bash
# Day 1: åŸ·è¡Œåˆ°ä¸€åŠæ™‚å®¹å™¨å´©æ½°ï¼ˆå·²å®Œæˆåˆ†é¡ A-Cï¼‰

# Day 2: é‡æ–°å•Ÿå‹•
await crawl_service.run_all(limit_per_platform=10, resume=True)

# ç³»çµ±è‡ªå‹•è·³éå·²å®Œæˆçš„ A-Cï¼Œæ¥çºŒåˆ†é¡ D
```

### å ´æ™¯ 3: é‡æ–°è™•ç†æŸå¹³å°

```python
# éœ€è¦é‡æ–°çˆ¬å–å¹³å° 104 çš„å…¨éƒ¨åˆ†é¡ï¼ˆè·³é resumeï¼‰
await crawl_service.run_platform(
    SourcePlatform.PLATFORM_104,
    max_jobs=20,
    resume=False  # ä¸è·³éå·²å®Œæˆåˆ†é¡ï¼Œå…¨éƒ¨é‡åš
)
```

### å ´æ™¯ 4: åƒ…è™•ç†æŸå€‹åˆ†é¡

```python
# åªçˆ¬å–å¹³å° 1111 çš„ç‰¹å®šåˆ†é¡ï¼ˆå¦‚ã€Œè»Ÿé«”å·¥ç¨‹ã€ï¼‰
await crawl_service.run_platform(
    SourcePlatform.PLATFORM_1111,
    target_cat_id="software_engineering",  # æŒ‡å®šåˆ†é¡
    max_jobs=50
)
```

---

## æ—¥èªŒè¼¸å‡ºç¯„ä¾‹

### åŸ·è¡Œæ—¥èªŒ

```json
{
  "event": "pipeline_started",
  "platform": "platform_104",
  "category_mode": "sequential",
  "resume": true,
  "timestamp": "2026-01-29T10:00:00Z"
}

{
  "event": "resume_mode_filtered",
  "platform": "platform_104",
  "crawled_before": 15,
  "remaining": 8,
  "timestamp": "2026-01-29T10:00:05Z"
}

{
  "event": "category_processing_start",
  "platform": "platform_104",
  "category_index": "1/8",
  "cat_id": "cat_001",
  "cat_name": "è»Ÿé«”å·¥ç¨‹",
  "timestamp": "2026-01-29T10:00:10Z"
}

{
  "event": "category_discovery_stats",
  "platform": "platform_104",
  "cat": "cat_001",
  "count": 45,
  "timestamp": "2026-01-29T10:00:12Z"
}

[... URL è™•ç† ...]

{
  "event": "category_processing_completed",
  "platform": "platform_104",
  "cat": "cat_001",
  "progress": "1/8",
  "timestamp": "2026-01-29T10:15:30Z"
}

{
  "event": "category_processing_start",
  "platform": "platform_104",
  "category_index": "2/8",
  "cat_id": "cat_002",
  "cat_name": "å¸‚å ´è¡ŒéŠ·",
  "timestamp": "2026-01-29T10:15:35Z"
}

[... é‡è¤‡ ...]

{
  "event": "pipeline_completed",
  "platform": "platform_104",
  "total_categories": 8,
  "timestamp": "2026-01-29T11:30:00Z"
}
```

---

## æ€§èƒ½å½±éŸ¿

### ååé‡å°æ¯”

| æŒ‡æ¨™ | ä¸¦è¡Œåˆ†é¡ | é †åºåˆ†é¡ | å‚™è¨» |
|------|---------|---------|------|
| **å–®å¹³å°è€—æ™‚** | 12 min | 12 min | ç¸½è€—æ™‚ç›¸åŒï¼ˆå— URL ç´šä½µç™¼å½±éŸ¿ï¼‰ |
| **é€²åº¦è¿½è¹¤** | âŒ ç²—ç³™ | âœ… ç²¾ç¢º | é †åºåŸ·è¡Œæ›´æ˜“è¿½è¹¤ |
| **æ¢å¾©èƒ½åŠ›** | âŒ å·® | âœ… å¼· | å¯ç²¾ç¢ºæ¥çºŒ |
| **é‹ç¶­æˆæœ¬** | ğŸ”´ é«˜ | ğŸŸ¢ ä½ | é †åºåŸ·è¡Œæ›´æ˜“ç›£æ§ |

### ååé‡è¨ˆç®—

```
ç¸½è·ç¼ºæ•¸ = 8 åˆ†é¡ Ã— 50 URL/åˆ†é¡ = 400 URL

ä¸¦è¡Œåˆ†é¡:
  [Categ A] â”
  [Categ B] â”œâ”€ ä¸¦è¡Œ (8 å€‹åˆ†é¡åŒæ™‚)
  [Categ C] â”¤  è€—æ™‚ = 12 min
  ...       â”˜
  å–® URL è€—æ™‚ â‰ˆ 1.8 sec
  
é †åºåˆ†é¡:
  [Categ A] â”€â”€â”€â”€â”€
  [Categ B] â”€â”€â”€â”€â”€    é †åº (ä¸€å€‹æ¥ä¸€å€‹)
  [Categ C] â”€â”€â”€â”€â”€    è€—æ™‚ = 12 min (ç›¸åŒï¼ä¿¡è™Ÿé‡æ§åˆ¶)
  ...
  å–® URL è€—æ™‚ â‰ˆ 1.8 sec (ç›¸åŒï¼)

çµè«–: ç¸½ååç›¸åŒï¼Œä½†è¿½è¹¤èƒ½åŠ›é¡¯è‘—æå‡
```

---

## éƒ¨ç½²æª¢æŸ¥æ¸…å–®

- [ ] ä»£ç¢¼ä¿®æ”¹å·²æ¸¬è©¦
- [ ] æ–°å¢ `get_crawled_categories()` æ–¹æ³•
- [ ] æ•¸æ“šåº«è¡¨ `tb_categories` æœ‰ `updated_at` æ¬„ä½
- [ ] æ—¥èªŒè¼¸å‡ºé©—è­‰ï¼ˆè¦‹ä¸Šè¿°ç¯„ä¾‹ï¼‰
- [ ] åŸ·è¡Œ `pytest test/sdd/test_sequential_execution.py`
- [ ] æ¸¬è©¦ resume æ©Ÿåˆ¶ï¼šåœæ­¢å¾Œå†å•Ÿå‹•
- [ ] é©—è­‰é€²åº¦è¨˜éŒ„æ­£ç¢º

---

## å›æ»¾æ–¹æ¡ˆ

è‹¥è¦å›åˆ°ä¸¦è¡Œåˆ†é¡æ¨¡å¼ï¼š

```python
# æ”¹å›èˆŠé‚è¼¯
cat_tasks = [process_category(cat) for cat in categories]
await asyncio.gather(*cat_tasks)
```

ä½† **ä¸å»ºè­°** å›æ»¾ï¼Œå› ç‚ºé †åºåŸ·è¡Œæä¾›äº†æ›´å¥½çš„å¯è§€æ¸¬æ€§å’Œå®¹éŒ¯èƒ½åŠ›ã€‚

---

