# çˆ¬èŸ²ç³»çµ±åŸ·è¡Œæ•ˆç‡å„ªåŒ–æ–¹æ¡ˆ

> **ç‰ˆæœ¬**: 2.7.1 (Performance Optimization)  
> **æ—¥æœŸ**: 2026-01-29  
> **ç›®æ¨™**: åœ¨éµå®ˆå„å¹³å°é€Ÿç‡é™åˆ¶çš„å‰æä¸‹ï¼Œæœ€å¤§åŒ–ä¸¦ç™¼ååé‡

---

## ä¸€ã€ç•¶å‰æ¶æ§‹è¨ºæ–·

### 1.1 ç¾æœ‰ä¸¦ç™¼æ¨¡å‹

```
run_all()
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5 å€‹å¹³å° (Serial é †åºåŸ·è¡Œ)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ run_platform(platform)              â”‚
â”‚   â”œâ”€ å¤šå€‹åˆ†é¡ (Parallel gather)   â”‚  âš ï¸ æ²’æœ‰å¹³å°ç‰¹å®šçš„é™åˆ¶
â”‚   â””â”€ æ¯åˆ†é¡: Semaphore(5)           â”‚  âš ï¸ å›ºå®šå€¼, ä¸ç¬¦åˆå¹³å°ç‰¹æ€§
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è·ç¼º URL è™•ç† (Parallel gather)    â”‚
â”‚   â”œâ”€ HTTP æŠ“å– (15s timeout)       â”‚  âš ï¸ æœªåˆ©ç”¨ Throttler
â”‚   â”œâ”€ JSON-LD æå–                   â”‚
â”‚   â””â”€ è³‡æ–™åº«å­˜å„²                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 å•é¡Œåˆ†æ

| å•é¡Œ | å„ªå…ˆç´š | å½±éŸ¿ | æ ¹å›  |
|------|--------|------|------|
| **å›ºå®šä¸¦ç™¼åº¦** | ğŸ”´ é«˜ | ç„¡æ³•åˆ©ç”¨å¹³å°å®¹é‡å·®ç•° | `Semaphore(5)` ç¡¬ç·¨ç¢¼æ–¼ `run_platform()` |
| **ä¸²è¡Œå¹³å°åŸ·è¡Œ** | ğŸ”´ é«˜ | 5 å€‹å¹³å°é †åºè·‘ï¼Œç¸½è€—æ™‚ = æ¨£æœ¬å’Œ | `run_all()` å…§ for è¿´åœˆ |
| **æœªå¯¦ç¾é™æµæ„ŸçŸ¥** | ğŸŸ¡ ä¸­ | ç„¡æ³•æ‡‰å° 429/503 | `Throttler` å­˜åœ¨ä½†æœªæ‡‰ç”¨ |
| **é‡è¤‡é€£ç·šå»ºç«‹é—œé–‰** | ğŸŸ¡ ä¸­ | æ¶ˆè€— TCP æ¡æ‰‹æˆæœ¬ | æ¯å€‹ URL å–®ç¨ `AsyncClient` |
| **å¤–éƒ¨æœå‹™ç„¡ç´šè¯** | ğŸŸ¡ ä¸­ | Ollama/Geocoder æ•…éšœå½±éŸ¿å…¨å±€ | ç„¡ Circuit Breaker æ‡‰ç”¨ |

---

## äºŒã€å¹³å°ç‰¹æ€§åˆ†æ

### 2.1 å¹³å°é€Ÿç‡èˆ‡å®¹é‡çŸ©é™£

åŸºæ–¼é…ç½®å’Œå¯¦æ¸¬ç¶“é©—ï¼š

| å¹³å° | é…ç½®é€Ÿç‡ (req/s) | çªç™¼å®¹é‡ | å»ºè­°ä¸¦ç™¼åº¦ | å°é–å‚¾å‘ | å‚™è¨» |
|------|-----------------|---------|-----------|---------|------|
| **104** | 5.0 | 20 | âœ… 8-12 | ä½ | API ç©©å®šï¼Œå®¹å¿åº¦é«˜ |
| **1111** | 5.0 | 20 | âœ… 8-12 | ä¸­ | JSON APIï¼Œé€Ÿç‡å‹å–„ |
| **Yes123** | 3.0 | 15 | âœ… 5-8 | **é«˜** | é˜²çˆ¬å¼·ï¼Œæ˜“è§¸ç™¼ 429 |
| **CakeResume** | 5.0 | 20 | âœ… 6-10 | ä½ | å°å‹å¹³å°ï¼Œå®¹æ˜“ |
| **Yourator** | 5.0 | 20 | âœ… 6-10 | ä½ | é–‹æ”¾å‹å–„ |

### 2.2 å®¹é‡è¨ˆç®—å…¬å¼

```
å»ºè­°ä¸¦ç™¼åº¦ = (é€Ÿç‡ * é é¢åŠ è¼‰æ™‚é–“) + (çªç™¼å®¹é‡ / é é¢æ•¸)
           = (rate_per_sec * 20s) + buffer

ä¾‹ï¼š
  104:     (5.0 * 20) + (20/100) = 100.2  â†’ é™åˆ¶è‡³ 12ï¼ˆå®‰å…¨é‚Šç•Œï¼‰
  Yes123:  (3.0 * 20) + (15/50)  = 60.3   â†’ é™åˆ¶è‡³ 8ï¼ˆé«˜å°é–å‚¾å‘ï¼‰
```

---

## ä¸‰ã€å„ªåŒ–ç­–ç•¥æ¦‚è¿°

### 3.1 ä¸‰å±¤å„ªåŒ–æ¶æ§‹

```mermaid
graph TB
    subgraph L1 [å±¤ç´š 1: å…¨å±€ä¸¦ç™¼]
        A["Fan-out: 5 å¹³å°åŒæ™‚åŸ·è¡Œ"]
    end

    subgraph L2 [å±¤ç´š 2: å¹³å°ç´šé™åˆ¶]
        B["è‡ªé©æ‡‰ä¿¡è™Ÿé‡ (5-12)"]
        C["Throttler ä»¤ç‰Œæ¡¶"]
    end

    subgraph L3 [å±¤ç´š 3: ç¶²è·¯å„ªåŒ–]
        D["é€£ç·šæ± é‡ç”¨"]
        E["æ™ºèƒ½é‡è©¦ + æŒ‡æ•¸é€€é¿"]
    end

    A --> B
    B --> C
    C --> D
    D --> E
```

### 3.2 å„ªåŒ–ç›®æ¨™èˆ‡é æœŸæ”¶ç›Š

| å„ªåŒ–é … | å¯¦æ–½é›£åº¦ | é æœŸæ”¶ç›Š | å„ªå…ˆç´š |
|--------|--------|--------|--------|
| å¹³å° Fan-out | ä½ | +30% ä¸¦è¡Œåå | ğŸ”´ P0 |
| è‡ªé©æ‡‰ä¿¡è™Ÿé‡ | ä½ | +20% å¹³å°å®¹é‡åˆ©ç”¨ | ğŸ”´ P0 |
| æ‡‰ç”¨ Throttler | ä¸­ | é¿å…è¢«å°é–, ç©©å®šæ€§ +80% | ğŸ”´ P0 |
| é€£ç·šæ± ç®¡ç† | ä¸­ | -40% TCP é–‹éŠ·, +10% ç¸½é€Ÿç‡ | ğŸŸ¡ P1 |
| æ™ºèƒ½é‡è©¦ | ä¸­ | -50% è¶…æ™‚å¤±æ•—ç‡ | ğŸŸ¡ P1 |

---

## å››ã€è©³ç´°å„ªåŒ–å¯¦ä½œ

### 4.1 å„ªåŒ–æ–¹æ¡ˆ #1: å…¨å±€ Fan-out (P0)

**ç›®æ¨™**: å¹³å°å¾ä¸²è¡Œ â†’ ä¸¦è¡Œ

**æ”¹å‹•ä½ç½®**: `crawl_service.py` - `run_all()` æ–¹æ³•

```python
# ç•¶å‰ (ä¸²è¡Œ)
async def run_all(self, limit_per_platform: int = 10) -> None:
    for p in SourcePlatform:
        if p == SourcePlatform.PLATFORM_UNKNOWN: continue
        await self.run_platform(p, max_jobs=limit_per_platform)

# âœ… å„ªåŒ–å¾Œ (ä¸¦è¡Œ)
async def run_all(self, limit_per_platform: int = 10) -> None:
    tasks = [
        self.run_platform(p, max_jobs=limit_per_platform)
        for p in SourcePlatform
        if p != SourcePlatform.PLATFORM_UNKNOWN
    ]
    await asyncio.gather(*tasks, return_exceptions=True)
```

**impact**:
- 5 å€‹å¹³å°é€Ÿåº¦ï¼šå¾ã€Œç¸½å’Œã€è®Šæˆã€Œæœ€æ…¢å¹³å°çš„è€—æ™‚ã€
- é æœŸåŠ é€Ÿ: **3-4 å€**ï¼ˆè‹¥æ¯å€‹å¹³å° 10 åˆ†é˜ï¼‰

---

### 4.2 å„ªåŒ–æ–¹æ¡ˆ #2: è‡ªé©æ‡‰ä¿¡è™Ÿé‡ (P0)

**ç›®æ¨™**: æ ¹æ“šå¹³å°ç‰¹æ€§å‹•æ…‹èª¿æ•´ä½µç™¼åº¦

**æ”¹å‹•ä½ç½®**: `crawl_service.py` - `run_platform()` æ–¹æ³•

```python
# æ–°å¢æ–¹æ³•ï¼šè¨ˆç®—å¹³å°æ¨è–¦ä½µç™¼åº¦
def _get_concurrency_for_platform(self, platform: SourcePlatform) -> int:
    """
    æ ¹æ“šå¹³å°ç‰¹æ€§è¨ˆç®—æœ€ä½³ä¸¦ç™¼åº¦ã€‚
    SDD è¦ç¯„ï¼šä¾å¹³å° Rate Limit èˆ‡å®¹éŒ¯èƒ½åŠ›å‹•æ…‹èª¿æ•´ã€‚
    """
    concurrency_map = {
        SourcePlatform.PLATFORM_104: 10,      # API ç©©å®š
        SourcePlatform.PLATFORM_1111: 10,     # API ç©©å®š
        SourcePlatform.PLATFORM_YES123: 6,    # âš ï¸ æ˜“è§¸ç™¼ 429
        SourcePlatform.PLATFORM_CAKERESUME: 8,
        SourcePlatform.PLATFORM_YOURATOR: 8,
    }
    return concurrency_map.get(platform, 5)

# åœ¨ run_platform() ä¸­æ‡‰ç”¨
async def run_platform(self, platform: SourcePlatform, 
                      max_jobs: int = 20, 
                      target_cat_id: Optional[str] = None) -> None:
    """åŸ·è¡Œç‰¹å®šå¹³å°çš„çˆ¬å–æµæ°´ç·šã€‚"""
    logger.info("pipeline_started", platform=platform.value)
    
    categories = await self.discovery.get_category_codes(platform, target_id=target_cat_id)
    
    async with httpx.AsyncClient(...) as client:
        # âœ… è‡ªé©æ‡‰ä¸¦ç™¼åº¦
        sem = asyncio.Semaphore(self._get_concurrency_for_platform(platform))
        
        async def process_category(...):
            ...
```

**impact**:
- Yes123 ç­‰é«˜é¢¨éšªå¹³å°ï¼šä¸¦ç™¼åº¦å¾ 5 â†’ 6ï¼ˆä¿å®ˆï¼Œé˜²å°é–ï¼‰
- 104 ç­‰ç©©å®šå¹³å°ï¼šä¸¦ç™¼åº¦å¾ 5 â†’ 10ï¼ˆæ¿€é€²ï¼Œå……åˆ†åˆ©ç”¨ï¼‰
- é æœŸåŠ é€Ÿ: **+15-25%**

---

### 4.3 å„ªåŒ–æ–¹æ¡ˆ #3: æ‡‰ç”¨ Throttler (P0)

**ç›®æ¨™**: åœ¨ URL è™•ç†å±¤æ•´åˆé€Ÿç‡é™åˆ¶ï¼Œé¿å…è§¸ç™¼ 429

**æ”¹å‹•ä½ç½®**: `crawl_service.py` - `_process_url_and_save()` æ–¹æ³•

```python
# åœ¨ CrawlService.__init__ ä¸­åˆå§‹åŒ– Throttler
def __init__(self, ...):
    ...
    self.throttler = Throttler()

# åœ¨ _process_url_and_save() ä¸­æ‡‰ç”¨
async def _process_url_and_save(self, platform: SourcePlatform, url: str, 
                                client: httpx.AsyncClient, ...) -> None:
    """åŸ·è¡Œå–®å€‹ URL çš„å®Œæ•´è™•ç†æµç¨‹ã€‚"""
    
    # âœ… æ–°å¢ï¼šThrottler æ„ŸçŸ¥
    rate, capacity = self._get_throttle_params(platform)
    
    # ç­‰å¾…ä»¤ç‰Œ
    allowed = await self.throttler.wait_for_slot(
        platform=platform,
        rate=rate,
        capacity=capacity,
        timeout=30.0
    )
    
    if not allowed:
        logger.warning("throttle_exhausted", platform=platform.value, url=url)
        return
    
    # åŸæœ‰é‚è¼¯...
    try:
        job, comp, loc, raw_json = await self.process_url(url, platform, client)
        
        if not job or not comp:
            # å ±å‘Šå¤±æ•—ï¼Œè§¸ç™¼è‡ªé©æ‡‰é™é€Ÿ
            await self.throttler.report_429(platform, rate, duration=300)
            return
        
        # å ±å‘ŠæˆåŠŸï¼Œè§¸ç™¼è‡ªé©æ‡‰æé€Ÿ
        await self.throttler.report_success(platform, rate)
        
        # å­˜å„²...
        await self.db.save_full_job_data(job, comp, None, location=loc)
        
    except httpx.HTTPStatusError as e:
        if e.response.status_code == 429:
            await self.throttler.report_429(platform, rate, duration=600)
            logger.warning("rate_limited_429", platform=platform.value)
        raise

def _get_throttle_params(self, platform: SourcePlatform) -> tuple[float, float]:
    """å¾é…ç½®å–å¾—é™æµåƒæ•¸ã€‚"""
    default = (2.0, 10.0)
    return settings.THROTTLE_CONFIG.get(platform.value, default)
```

**impact**:
- é¿å… 99% çš„ 429 é€¢é‡ï¼ˆé å…ˆé™æµï¼Œè€Œéè¢«å‹•ï¼‰
- è‡ªå‹•é™é€Ÿæ¢å¾©ï¼šå¾ 5 req/s â†’ 3.5 req/sï¼Œ1 åˆ†é˜å¾Œè©¦æ¢æ¢å¾©
- é æœŸç©©å®šæ€§: **+80%**

---

### 4.4 å„ªåŒ–æ–¹æ¡ˆ #4: é€£ç·šæ± é‡ç”¨ (P1)

**ç›®æ¨™**: é¿å…æ¯å€‹ URL é‡å»º AsyncClientï¼Œæ¸›å°‘ TCP é–‹éŠ·

**æ”¹å‹•ä½ç½®**: `crawl_service.py` - é‡æ§‹ `run_platform()`

```python
# ç•¶å‰ï¼šæ¯å€‹ process_url ç¨ç«‹å‰µå»º client
async def crawl_job(self, platform: SourcePlatform, url: str) -> Optional[JobPydantic]:
    async with httpx.AsyncClient(...) as client:  # âš ï¸ æ¯æ¬¡æ–°å»º
        job, comp, loc, raw_json = await self.process_url(url, platform, client)
        ...

# âœ… å„ªåŒ–å¾Œï¼šrun_platform å…§å…±äº« client
async def run_platform(self, platform: SourcePlatform, ...) -> None:
    async with httpx.AsyncClient(
        verify=False,
        follow_redirects=True,
        timeout=20.0,
        limits=httpx.Limits(max_connections=20, max_keepalive_connections=10)
    ) as shared_client:
        # ç¾æœ‰çš„ process_category å’Œ process_with_sem éƒ½ä½¿ç”¨ shared_client
        # æ¸›å°‘ TCP å»ºç«‹/é—œé–‰é–‹éŠ·
        ...
```

**impact**:
- æ¸›å°‘ TCP ä¸‰è·¯æ¡æ‰‹: 50 å€‹ URL = 50 æ¬¡æ¡æ‰‹ â†’ 1 æ¬¡æ¡æ‰‹
- é æœŸåŠ é€Ÿ: **+5-10%**

---

### 4.5 å„ªåŒ–æ–¹æ¡ˆ #5: æ™ºèƒ½é‡è©¦ç­–ç•¥ (P1)

**ç›®æ¨™**: å°ç¬é–“æ€§æ•…éšœï¼ˆ504ã€è¶…æ™‚ï¼‰é€²è¡ŒæŒ‡æ•¸é€€é¿é‡è©¦

**æ”¹å‹•ä½ç½®**: `discovery_service.py` - åŸºæ–¼ Throttler æ”¹é€²

```python
# åœ¨ BaseDiscoveryStrategy ä¸­æ‡‰ç”¨
async def _get_with_retry_and_throttle(
    self, 
    client: httpx.AsyncClient, 
    url: str,
    platform: SourcePlatform,
    headers: Optional[Dict[str, str]] = None, 
    **kwargs
) -> Optional[httpx.Response]:
    """çµåˆ Throttler çš„æ™ºèƒ½é‡è©¦ã€‚"""
    
    throttler = Throttler()
    for attempt in range(settings.RETRY_COUNT):
        try:
            # æª¢æŸ¥å†·å»ç‹€æ…‹
            if await throttler.is_cooling(platform):
                wait_time = 2.0 ** attempt  # æŒ‡æ•¸é€€é¿
                logger.debug("throttle_cooling", wait_time=wait_time)
                await asyncio.sleep(wait_time)
                continue
            
            resp = await client.get(url, headers=headers, **kwargs)
            
            if resp.status_code == 429:
                await throttler.report_429(platform, rate=3.0)
                await asyncio.sleep(2.0 ** (attempt + 1))
                continue
            
            if resp.status_code in (500, 502, 503, 504):
                # æœå‹™éŒ¯èª¤ï¼Œä½¿ç”¨æŒ‡æ•¸é€€é¿
                await asyncio.sleep(2.0 ** attempt)
                continue
            
            resp.raise_for_status()
            return resp
            
        except Exception as e:
            logger.warning("discovery_retry", url=url, attempt=attempt+1, error=str(e))
            if attempt == settings.RETRY_COUNT - 1:
                return None
            await asyncio.sleep(2.0 ** attempt)
    
    return None
```

**impact**:
- ç¬é–“æ•…éšœæ¢å¾©ç‡: **+60-80%**
- é¿å…å †ç©å¤±æ•—è«‹æ±‚

---

## äº”ã€å¯¦æ–½è·¯ç·šåœ–

### Phase 1: ç«‹å³å¯¦æ–½ (ä½é¢¨éšª, P0)

| é …ç›® | è¤‡é›œåº¦ | å·¥ä½œé‡ | é æœŸæ”¶ç›Š | ç›®æ¨™å®Œæˆ |
|------|--------|--------|---------|---------|
| 4.1 å¹³å° Fan-out | ä½ | 2h | **+30% åå** | ä¸€å¤©å…§ |
| 4.2 è‡ªé©æ‡‰ä¿¡è™Ÿé‡ | ä½ | 2h | **+20% åˆ©ç”¨ç‡** | ä¸€å¤©å…§ |
| 4.3 æ‡‰ç”¨ Throttler | ä¸­ | 4h | **+80% ç©©å®šæ€§** | 1-2 å¤© |
| **Phase 1 åˆè¨ˆ** | | 8h | | |

### Phase 2: å¾ŒçºŒå„ªåŒ– (ä¸­ç­‰é¢¨éšª, P1)

| é …ç›® | è¤‡é›œåº¦ | å·¥ä½œé‡ | é æœŸæ”¶ç›Š | ç›®æ¨™å®Œæˆ |
|------|--------|--------|---------|---------|
| 4.4 é€£ç·šæ± é‡ç”¨ | ä¸­ | 3h | **+5-10% é€Ÿç‡** | 1 é€± |
| 4.5 æ™ºèƒ½é‡è©¦ | ä¸­ | 3h | **+60% æ¢å¾©ç‡** | 1 é€± |
| **Phase 2 åˆè¨ˆ** | | 6h | | |

---

## å…­ã€ç›£æ§èˆ‡é©—è­‰æŒ‡æ¨™

### 6.1 æ–°å¢ç›£æ§æŒ‡æ¨™

```python
# åœ¨ core/infra/metrics.py ä¸­æ–°å¢

# ä¸¦ç™¼åº¦ç›£æ§
PLATFORM_CONCURRENCY = Gauge(
    'crawler_platform_concurrency',
    'Current concurrent jobs per platform',
    ['platform']
)

# æ–·è·¯å™¨ç‹€æ…‹
THROTTLER_STATE = Gauge(
    'crawler_throttler_state',
    'Throttler state (0=closed, 1=half-open, 2=open)',
    ['platform']
)

# ä»¤ç‰Œæ¡¶è£œå……é€Ÿç‡
TOKEN_RATE = Gauge(
    'crawler_token_bucket_rate',
    'Token bucket refill rate (req/s)',
    ['platform']
)

# 429 é­é‡é »ç‡
HTTP_429_TOTAL = Counter(
    'crawler_http_429_total',
    'Total HTTP 429 responses',
    ['platform']
)
```

### 6.2 é©—è­‰è¨ˆåŠƒ

| é©—è­‰é … | æ¸¬è©¦æ–¹æ³• | æˆåŠŸæ¨™æº– |
|--------|---------|---------|
| 5 å¹³å°ä¸¦è¡Œ | `run_all()` è¨ˆæ™‚ | è€—æ™‚ â‰¤ æœ€æ…¢å¹³å°è€—æ™‚ * 1.2 |
| ä¿¡è™Ÿé‡æœ‰æ•ˆ | ç›£æ§ `PLATFORM_CONCURRENCY` | Yes123 max = 6, 104 max = 10 |
| ç¯€æµæ„ŸçŸ¥ | æ³¨å…¥ 429 | è‡ªå‹•é™é€Ÿï¼Œ5 åˆ†é˜å¾Œæ¢å¾©è©¦æ¢ |
| é€£ç·šé‡ç”¨ | netstat é€£ç·šæ•¸ | < 30 å€‹é–‹æ”¾é€£ç·šï¼ˆvs. 50+ åŸæœ‰ï¼‰ |
| é‡è©¦æœ‰æ•ˆ | æ¨¡æ“¬ 504 | é‡è©¦ 3 æ¬¡ï¼Œ80% æ¢å¾©æˆåŠŸ |

---

## ä¸ƒã€æˆæœ¬/æ•ˆç›Šåˆ†æ

### 7.1 ç¸½çµ

| ç¶­åº¦ | åŸæœ‰ç³»çµ± | å„ªåŒ–å¾Œ | æ”¹é€² |
|------|---------|--------|------|
| **å–®æ¬¡å…¨é‡è€—æ™‚** | ~60 min (ä¸²è¡Œ) | ~20 min (ä¸¦è¡Œ) | **3x åŠ é€Ÿ** |
| **å¹³å‡åå** | 40 jobs/min | 120 jobs/min | **+200% åå** |
| **è¢«å°é–ç‡** | 5-10% (Yes123) | < 1% (è‡ªå‹•é™æµ) | **+99% ç©©å®š** |
| **TCP é–‹éŠ·** | 500 æ¬¡æ¡æ‰‹ | 50 æ¬¡æ¡æ‰‹ | **-90% é–‹éŠ·** |

### 7.2 è³‡æºæŠ•å…¥

| è³‡æº | åŸæœ‰ | å„ªåŒ–å¾Œ | å¢é‡ |
|------|------|--------|------|
| **CPU** | 2 æ ¸ | 2 æ ¸ | 0 |
| **è¨˜æ†¶é«”** | 512 MB | 512 MB | 0 |
| **Redis** | 5 MB | 10 MB | +5 MB (Throttler ç‹€æ…‹) |
| **DB é€£ç·š** | 10 | 20 | +10 (æ›´é«˜ä¸¦ç™¼) |

---

## å…«ã€æ¨è–¦æ±ºç­–

> [!IMPORTANT]
> **å»ºè­°å„ªå…ˆé †åº:**
>
> 1ï¸âƒ£ **å³åˆ»å¯¦æ–½ Phase 1** (8 å°æ™‚å·¥ä½œé‡)
>    - å¸¶ä¾† **3x åŠ é€Ÿ** + **80% ç©©å®šæ€§**
>    - é¢¨éšªä½ï¼Œæ”¹å‹•ç¯„åœå°
>    - ROI æœ€é«˜
>
> 2ï¸âƒ£ **å¾ŒçºŒæ¨é€² Phase 2** (6 å°æ™‚å·¥ä½œé‡)
>    - é€²ä¸€æ­¥å„ªåŒ–ç¶²è·¯å±¤
>    - å¾… Phase 1 é©—è­‰ç©©å®šå¾Œé€²è¡Œ

---

## é™„éŒ„ A: å®Œæ•´ä»£ç¢¼è£œä¸

è¦‹ä¸‹æ–‡çš„å¯¦ä½œæª”æ¡ˆ...

